{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 1: Floating-point arithmetic, vector norms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Syllabus\n",
    "\n",
    "**Today:** \n",
    "- Part 1: floating point, vector norms\n",
    "- Part 2: matrix norms and unitary matrices\n",
    "\n",
    "**Thursday:** SVD and Skeleton decomposition, matrix multiplication\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Representation of numbers\n",
    "\n",
    "- Real numbers represent quantities: probabilities, velocities, masses, ...\n",
    "\n",
    "- It is important to know, how they are represented in the computer, which only knows about bits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fixed point\n",
    "\n",
    "- The most straightforward format for the representation of real numbers is **fixed point** representation, also known as **Qm.n** format.\n",
    "\n",
    "- A **Qm.n** number is in the range $[-(2^m), 2^m - 2^{-n}]$, with resolution $2^{-n}$.\n",
    "\n",
    "- Total storage is $m + n + 1$ bits.\n",
    "\n",
    "- The range of numbers represented is fixed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Floating point\n",
    "The numbers in computer memory are typically represented as **floating point numbers** \n",
    "\n",
    "A floating point number is represented as  \n",
    "\n",
    "$$\\textrm{number} = \\textrm{significand} \\times \\textrm{base}^{\\textrm{exponent}},$$\n",
    "\n",
    "where *significand* is integer, *base* is positive integer  and *exponent* is integer (can be negative), i.e.\n",
    "\n",
    "$$ 1.2 = 12 \\cdot 10^{-1}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fixed vs Floating\n",
    "\n",
    "**Q**: What are the advantages/disadvantages of the fixed and floating points?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**A**:  In most cases, they work just fine.\n",
    "\n",
    "- However, fixed point represents numbers within specified range and controls **absolute** accuracy.\n",
    "\n",
    "- Floating point represent numbers with **relative** accuracy, and is suitable for the case when numbers in the computations have varying scale (i.e., $10^{-1}$ and $10^{5}$).\n",
    "\n",
    "- In practice, if speed is of no concern, use float32 or float64."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## IEEE 754\n",
    "In modern computers, the floating point representation is controlled by [IEEE 754 standard](https://en.wikipedia.org/wiki/IEEE_floating_point) which was published in **1985** and before that point different computers behaved differently with floating point numbers. \n",
    "\n",
    "IEEE 754 has:\n",
    "- Floating point representation (as described above), $(-1)^s \\times c \\times b^q$.\n",
    "- Two infinities, $+\\infty$ and $-\\infty$\n",
    "- Two kinds of **NaN**: a quiet NaN (**qNaN**) and signalling NaN (**sNaN**) \n",
    "    - qNaN does not throw exception in the level of floating point unit (FPU), until you check the result of computations\n",
    "    - sNaN value throws exception from FPU if you use corresponding variable. This type of NaN can be useful for initialization purposes\n",
    "    - C++11 proposes [standard interface](https://en.cppreference.com/w/cpp/numeric/math/nan) for creating different NaNs \n",
    "- Rules for **rounding**\n",
    "- Rules for $\\frac{0}{0}, \\frac{1}{-0}, \\ldots$\n",
    "\n",
    "Possible values are defined with\n",
    "- base $b$\n",
    "- accuracy $p$ - number of digits\n",
    "- maximum possible value $e_{\\max}$\n",
    "\n",
    "and have the following restrictions \n",
    "- $ 0 \\leq c \\leq b^p - 1$\n",
    "- $1 - e_{\\max} \\leq q + p - 1 \\leq e_{\\max}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The two most common format, single & double\n",
    "\n",
    "The two most common formats, called **binary32** and **binary64** (called also **single** and **double** formats). Recently, the format **binary16** plays important role in learning deep neural networks.\n",
    "\n",
    "| Name | Common Name | Base | Digits | Emin | Emax |\n",
    "|------|----------|----------|-------|------|------|\n",
    "|binary16| half precision | 2 | 11 | -14 | + 15 |\n",
    "|binary32| single precision | 2 | 24 | -126 | + 127 |  \n",
    "|binary64| double precision | 2 | 53 | -1022 | +1023 |  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Accuracy and memory\n",
    "\n",
    "The **relative accuracy** of single precision is $10^{-7}-10^{-8}$, while for double precision is $10^{-14}-10^{-16}$.\n",
    "\n",
    "<font color='red'> Crucial note 1: </font> A **float16** takes **2 bytes**, **float32** takes **4 bytes**, **float64**, or double precision, takes **8 bytes.**\n",
    "\n",
    "<font color='red'> Crucial note 2: </font> These are the only two floating point-types supported in hardware (float32 and float64).\n",
    "\n",
    "<font color='red'> Crucial note 3: </font> You should use **double precision** in CSE and **float** on GPU/Data Science.\n",
    "\n",
    "\n",
    "Also, half precision can be useful in training deep neural network, see this [paper](https://arxiv.org/pdf/1905.12334.pdf).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How does numbder represrentation format affect training of neural networks (NN)?\n",
    "\n",
    "- Weights in layers (fully-connected, convolutional, activation functions) can be stored with different accuracies\n",
    "- It is important to improve energy efficiency of the devices that are used to train NNs\n",
    "- Project [DeepFloat](https://github.com/facebookresearch/deepfloat) from Facebook demonstrates how re-develop floating point operations in a way to ensure efficiency in training NNs, more details see in this [paper](https://arxiv.org/pdf/1811.01721.pdf)\n",
    "- Affect of the real number srepresentation on the gradients of activation functions\n",
    "\n",
    "<img width=500, src=\"./grad_norm_fp16.png\">\n",
    "\n",
    "- And on the learning curves\n",
    "\n",
    "<img width=500, src=\"./train_val_curves.png\">\n",
    "\n",
    "Plots are taken from [this paper](https://arxiv.org/pdf/1710.03740.pdf%EF%BC%89%E3%80%82)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Alternative to the IEEE 754 standard\n",
    "\n",
    "Issues in IEEE 754:\n",
    "- overflow to infinity or zero\n",
    "- many different NaNs\n",
    "- invisible rounding errors\n",
    "- accuracy is very high or very poor\n",
    "- subnormal numbers â€“ numbers between 0 and minimal possible represented number, i.e. significand starts from zero\n",
    "\n",
    "Concept of **posits** can replace floating point numbers, see [this paper](http://www.johngustafson.net/pdfs/BeatingFloatingPoint.pdf)\n",
    "\n",
    "<img width=600 src=\"./posit.png\">\n",
    "\n",
    "- represent numbers with some accuracy, but provide limits of changing\n",
    "- no overflows!\n",
    "- example of a number representation \n",
    "\n",
    "<img width=600 src=\"./posit_example.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Division accuracy demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9259246\n",
      "0.1040364727377892\n",
      "-5.9604645e-08\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "#c = random.random()\n",
    "#print(c)\n",
    "c = np.float32(0.925924589693)\n",
    "print(c)\n",
    "a = np.float32(8.9)\n",
    "b = np.float32(c / a)\n",
    "print('{0:10.16f}'.format(b))\n",
    "print(a * b - c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Square root accuracy demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0000001468220603\n"
     ]
    }
   ],
   "source": [
    "#a = np.array(1.585858585887575775757575e-5, dtype=np.float)\n",
    "a = np.float32(5.0)\n",
    "b = np.sqrt(a)\n",
    "print('{0:10.16f}'.format(b ** 2 - a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exponent accuracy demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "a = np.array(2.28827272710, dtype=np.float32)\n",
    "b = np.exp(a)\n",
    "print(np.log(b) - a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary of demos\n",
    "\n",
    "- For some values the inverse functions give exact answers\n",
    "- The relative accuracy should be preserved due to the IEEE standard\n",
    "- Does not hold for many modern GPU\n",
    "- More details about adoptation of IEEE 754 standard for GPU you can find [here](https://docs.nvidia.com/cuda/floating-point/index.html#considerations-for-heterogeneous-world) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Loss of significance\n",
    "\n",
    "- Many operations lead to the loss of digits [loss of significance](https://en.wikipedia.org/wiki/Loss_of_significance)\n",
    "- For example, it is a bad idea to subtract two big numbers that are close, the difference will have fewer correct digits\n",
    "- This is related to algorithms and their properties (forward/backward stability), which we will discuss later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summation algorithm\n",
    "\n",
    "However, the rounding errors can depend on the algorithm.\n",
    "\n",
    "- Consider the simplest problem: given $n$ floating point numbers $x_1, \\ldots, x_n$  \n",
    "\n",
    "- Compute their sum\n",
    "\n",
    "$$S = \\sum_{i=1}^n x_i = x_1 + \\ldots + x_n.$$\n",
    "\n",
    "- The simplest algorithm is to add one-by-one \n",
    "\n",
    "- What is the actual error for such algorithm? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Na&iuml;ve algorithm\n",
    "\n",
    "Na&iuml;ve algorithm adds numbers one-by-one: \n",
    "\n",
    "$$y_1 = x_1, \\quad y_2 = y_1 + x_2, \\quad y_3 = y_2 + x_3, \\ldots.$$\n",
    "\n",
    "- The **worst-case** error is then proportional to $\\mathcal{O}(n)$, while **mean-squared** error is $\\mathcal{O}(\\sqrt{n})$.\n",
    "\n",
    "- The **Kahan algorithm** gives the worst-case error bound $\\mathcal{O}(1)$ (i.e., independent of $n$).  \n",
    "\n",
    "- <font color='red'> Can you find the $\\mathcal{O}(\\log n)$ algorithm? </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Kahan summation\n",
    "The following algorithm gives $2 \\varepsilon + \\mathcal{O}(n \\varepsilon^2)$ error, where $\\varepsilon$ is the machine precision.\n",
    "```python\n",
    "s = 0\n",
    "c = 0\n",
    "for i in range(len(x)):\n",
    "    y = x[i] - c\n",
    "    t = s + y\n",
    "    c = (t - s) - y\n",
    "    s = t\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in np sum: 1.9e-04\n",
      "Error in Kahan sum: -1.3e-07\n",
      "Error in dumb sum: -1.0e-02\n",
      "Error in math fsum: 1.3e-10\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from numba import jit\n",
    "\n",
    "n = 10 ** 8\n",
    "sm = 1e-10\n",
    "x = np.ones(n, dtype=np.float32) * sm\n",
    "x[0] = 1.0\n",
    "true_sum = 1.0 + (n - 1)*sm\n",
    "approx_sum = np.sum(x)\n",
    "math_fsum = math.fsum(x)\n",
    "\n",
    "\n",
    "@jit(nopython=True)\n",
    "def dumb_sum(x):\n",
    "    s = np.float32(0.0)\n",
    "    for i in range(len(x)):\n",
    "        s = s + x[i]\n",
    "    return s\n",
    "\n",
    "@jit(nopython=True)\n",
    "def kahan_sum(x):\n",
    "    s = np.float32(0.0)\n",
    "    c = np.float32(0.0)\n",
    "    for i in range(len(x)):\n",
    "        y = x[i] - c\n",
    "        t = s + y\n",
    "        c = (t - s) - y\n",
    "        s = t\n",
    "    return s\n",
    "k_sum = kahan_sum(x)\n",
    "d_sum = dumb_sum(x)\n",
    "print('Error in np sum: {0:3.1e}'.format(approx_sum - true_sum))\n",
    "print('Error in Kahan sum: {0:3.1e}'.format(k_sum - true_sum))\n",
    "print('Error in dumb sum: {0:3.1e}'.format(d_sum - true_sum))\n",
    "print('Error in math fsum: {0:3.1e}'.format(math_fsum - true_sum))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## More complicated example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "test_list = [1, 1e20, 1, -1e20]\n",
    "print(math.fsum(test_list))\n",
    "print(np.sum(test_list))\n",
    "print(1 + 1e20 + 1 - 1e20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary of floating-point \n",
    "You should be really careful with floating point, since it may give you incorrect answers due to rounding-off errors.\n",
    "\n",
    "For many standard algorithms, the stability is well-understood and problems can be easily detected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Vectors\n",
    "- In NLA we typically work not with **numbers**, but with **vectors**\n",
    "- Recall that a vector in a fixed basis of size $n$ can be represented as a 1D array with $n$ numbers \n",
    "- Typically, it is considered as an $n \\times 1$ matrix (**column vector**)\n",
    "\n",
    "**Example:** \n",
    "Polynomials with degree $\\leq n$ form a linear space. \n",
    "Polynomial $ x^3 - 2x^2 + 1$ can be considered as a vector $\\begin{bmatrix}1 \\\\ -2 \\\\ 0 \\\\ 1\\end{bmatrix}$ in the basis $\\{x^3, x^2, x, 1\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Vector norm\n",
    "\n",
    "- Vectors typically provide an (approximate) description of a physical (or some other) object \n",
    "\n",
    "- One of the main question is **how accurate** the approximation is (1%, 10%)\n",
    "\n",
    "- What is an acceptable representation, of course, depends on the particular applications. For example:\n",
    "    - In partial differential equations accuracies $10^{-5} - 10^{-10}$ are the typical case\n",
    "    - In data-based applications sometimes an error of $80\\%$ is ok, since the interesting signal is corrupted by a huge noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Distances and norms\n",
    "\n",
    "- Norm is a **qualitative measure of smallness of a vector** and is typically denoted as $\\Vert x \\Vert$.\n",
    "\n",
    "The norm should satisfy certain properties:\n",
    "\n",
    "- $\\Vert \\alpha x \\Vert = |\\alpha| \\Vert x \\Vert$\n",
    "- $\\Vert x + y \\Vert \\leq \\Vert x \\Vert + \\Vert y \\Vert$ (triangle inequality)\n",
    "- If $\\Vert x \\Vert = 0$ then $x = 0$\n",
    "\n",
    "The distance between two vectors is then defined as\n",
    "\n",
    "$$ d(x, y) = \\Vert x - y \\Vert. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Standard norms\n",
    "The most well-known and widely used norm is **euclidean norm**:\n",
    "\n",
    "$$\\Vert x \\Vert_2 = \\sqrt{\\sum_{i=1}^n |x_i|^2},$$\n",
    "\n",
    "which corresponds to the distance in our real life. If the vectors have complex elements, we use their modulus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## $p$-norm\n",
    "Euclidean norm, or $2$-norm, is a subclass of an important class of $p$-norms:\n",
    "\n",
    "$$ \\Vert x \\Vert_p = \\Big(\\sum_{i=1}^n |x_i|^p\\Big)^{1/p}. $$\n",
    "\n",
    "There are two very important special cases:\n",
    "- Infinity norm, or Chebyshev norm is defined as the maximal element: \n",
    "\n",
    "$$ \\Vert x \\Vert_{\\infty} = \\max_i | x_i| $$\n",
    "\n",
    "<img src=\"chebyshev.jpeg\" style=\"height: 1%\">\n",
    "\n",
    "- $L_1$ norm (or **Manhattan distance**) which is defined as the sum of modules of the elements of $x$: \n",
    "\n",
    "$$ \\Vert x \\Vert_1 = \\sum_i |x_i| $$\n",
    "  \n",
    "<img src=\"manhattan.jpeg\" style=\"height\">  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We will give examples where $L_1$ norm is very important: it all relates to the **compressed sensing** methods \n",
    "that emerged in the mid-00s as one of the most popular research topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Equivalence of the norms\n",
    "All norms are equivalent in the sense that\n",
    "\n",
    "$$ C_1 \\Vert x \\Vert_* \\leq  \\Vert x \\Vert_{**} \\leq C_2 \\Vert x \\Vert_* $$  \n",
    "\n",
    "for some positive constants $C_1(n), C_2(n)$, $x \\in \\mathbb{R}^n$ for any pairs of norms $\\Vert \\cdot \\Vert_*$ and $\\Vert \\cdot \\Vert_{**}$. The equivalence of the norms basically means that if the vector is small in one norm, it is small in another norm. However, the constants can be large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Computing norms in Python\n",
    "\n",
    "The NumPy package has all you need for computing norms: ```np.linalg.norm``` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative error in L1 norm: 0.0008175686950799947\n",
      "Relative error in L2 norm: 0.0009821793837744047\n",
      "Relative error in Chebyshev norm: 0.0023120134678956987\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "n = 100\n",
    "a = np.ones(n)\n",
    "b = a + 1e-3 * np.random.randn(n)\n",
    "print('Relative error in L1 norm:', np.linalg.norm(a - b, 1) / np.linalg.norm(b, 1))\n",
    "print('Relative error in L2 norm:', np.linalg.norm(a - b) / np.linalg.norm(b))\n",
    "print('Relative error in Chebyshev norm:', np.linalg.norm(a - b, np.inf) / np.linalg.norm(b, np.inf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Unit disks in different norms\n",
    "\n",
    "- A unit disk is a set of point such that $\\Vert x \\Vert \\leq 1$\n",
    "- For the euclidean norm a unit disk is a usual disk\n",
    "- For other norms unit disks look very different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Unit disk in the p-th norm, $p=1$')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "p = 1 # Which norm do we use\n",
    "M = 40000 # Number of sampling points\n",
    "a = np.random.randn(M, 2)\n",
    "b = []\n",
    "for i in range(M):\n",
    "    if np.linalg.norm(a[i, :], p) <= 1:\n",
    "        b.append(a[i, :])\n",
    "b = np.array(b)\n",
    "plt.plot(b[:, 0], b[:, 1], '.')\n",
    "plt.axis('equal')\n",
    "plt.title('Unit disk in the p-th norm, $p={0:}$'.format(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why $L_1$-norm can be important?\n",
    "\n",
    "$L_1$ norm, as it was discovered quite recently, plays an important role in **compressed sensing**. \n",
    "\n",
    "The simplest formulation of the considered problem is as follows:\n",
    "\n",
    "- You have some observations $f$ \n",
    "- You have a linear model $Ax = f$, where $A$ is an $n \\times m$ matrix, $A$ is **known**\n",
    "- The number of equations, $n$, is less than the number of unknowns, $m$\n",
    "\n",
    "The question: can we find the solution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The solution is obviously non-unique, so a natural approach is to find the solution that is minimal in the certain sense:\n",
    "\n",
    "\\begin{align*}\n",
    "& \\Vert x \\Vert \\rightarrow \\min_x \\\\\n",
    "\\mbox{subject to } & Ax = f\n",
    "\\end{align*}\n",
    "\n",
    "- Typical choice of $\\Vert x \\Vert = \\Vert x \\Vert_2$ leads to the **linear least squares problem** (and has been used for ages).  \n",
    "\n",
    "- The choice $\\Vert x \\Vert = \\Vert x \\Vert_1$ leads to the [**compressed sensing**](https://en.wikipedia.org/wiki/Compressed_sensing)\n",
    "- It typically yields the **sparsest solution**  \n",
    "\n",
    "[A short demo](tv-denoising-demo.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is a stable algorithm?\n",
    "\n",
    "And we finalize the lecture by the concept of **stability**.\n",
    "\n",
    "- Let $x$ be an object (for example, a vector) \n",
    "- Let $f(x)$ be the function (functional) you want to evaluate \n",
    "\n",
    "You also have a **numerical algorithm** ``alg(x)`` that actually computes **approximation** to $f(x)$.  \n",
    "\n",
    "The algorithm is called **forward stable**, if $$\\Vert alg(x) - f(x) \\Vert  \\leq \\varepsilon $$  \n",
    "\n",
    "The algorithm is called **backward stable**, if for any $x$ there is a close vector $x + \\delta x$ such that\n",
    "\n",
    "$$alg(x) = f(x + \\delta x)$$\n",
    "\n",
    "and $\\Vert \\delta x \\Vert$ is small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classical example\n",
    "A classical example is the **solution of linear systems of equations** using Gaussian elimination which is similar to LU factorization (more details later)\n",
    "\n",
    "We consider the **Hilbert matrix** with the elements\n",
    "\n",
    "$$A = \\{a_{ij}\\}, \\quad a_{ij} = \\frac{1}{i + j + 1}, \\quad i,j = 0, \\ldots, n-1.$$\n",
    "\n",
    "And consider a linear system\n",
    "\n",
    "$$Ax = f.$$\n",
    "\n",
    "We will look into matrices in more details in the next lecture, and for linear systems in the upcoming weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.817592071494168\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "n = 500\n",
    "a = [[1.0/(i + j + 1) for i in range(n)] for j in range(n)] # Hilbert matrix\n",
    "A = np.array(a)\n",
    "rhs =  np.random.random(n)\n",
    "sol = np.linalg.solve(A, rhs)\n",
    "print(np.linalg.norm(A.dot(sol) - rhs)/np.linalg.norm(rhs))\n",
    "#plt.plot(sol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.287160152440011e-08\n"
     ]
    }
   ],
   "source": [
    "rhs =  np.ones(n)\n",
    "sol = np.linalg.solve(A, rhs)\n",
    "print(np.linalg.norm(A.dot(sol) - rhs)/np.linalg.norm(rhs))\n",
    "#plt.plot(sol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## More examples of instability\n",
    "\n",
    "How to compute the following functions in numerically stable manner?\n",
    "\n",
    "- $\\log(1 - \\tanh^2(x))$\n",
    "- $SoftMax(x)_j = \\dfrac{e^{x_j}}{\\sum\\limits_{i=1}^n e^{x_i}}$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original function: -inf\n",
      "Attempt imporove stability with add small constant: -13.815510557964274\n",
      "Use more numerically stable form: -598.6137056388801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alex/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: RuntimeWarning: divide by zero encountered in log\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "u = 300\n",
    "eps = 1e-6\n",
    "print(\"Original function:\", np.log(1 - np.tanh(u)**2))\n",
    "eps_add = np.log(1 - np.tanh(u)**2 + eps)\n",
    "print(\"Attempt imporove stability with add small constant:\", eps_add)\n",
    "print(\"Use more numerically stable form:\", np.log(4) - 2 * np.log(np.exp(-u) + np.exp(u)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan  0.  0.  0.  0.]\n",
      "[1. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alex/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: RuntimeWarning: invalid value encountered in true_divide\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "n = 5\n",
    "x = np.random.randn(n)\n",
    "x[0] = 1000\n",
    "print(np.exp(x) / np.sum(np.exp(x)))\n",
    "print(np.exp(x - np.max(x)) / np.sum(np.exp(x - np.max(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Take home message\n",
    "- Floating point  (double, single, number of bytes), rounding error\n",
    "- Norms are measures of smallness, used to compute the accuracy\n",
    "- $1$, $p$ and Euclidean norms \n",
    "- $L_1$ is used in compressed sensing as a surrogate for sparsity (later lectures) \n",
    "- Forward/backward error (and stability of algorithms)  (later lectures)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
